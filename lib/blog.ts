"use client"

// This is a simple mock implementation for blog posts
// In a real project, you would use a library like contentlayer or a CMS

export interface Post {
  slug: string
  title: string
  date: string
  excerpt: string
  content: string
  tags: string[]
  readingTime: number
}

// Mock blog posts data based on Aparna's actual articles and expertise
const posts: Post[] = [
  {
    slug: "perspectives-in-generative-ai-with-stanford-researchers",
    title: "Perspectives in Generative AI with Stanford Researchers",
    date: "2023-12-10",
    excerpt:
      "Insights from conversations with Stanford researchers on the current state and future of generative AI technologies.",
    content:
      "# Perspectives in Generative AI with Stanford Researchers\n\nGenerative AI has emerged as one of the most transformative technologies of our time. To better understand where this field is heading, I recently spoke with leading researchers from Stanford University.\n\nThis discussion was part of Pear VC's ongoing series exploring the cutting edge of AI research and its applications in industry.\n\n## Key Insights\n\n### Foundation Models: The New Computing Paradigm\n\nAccording to Stanford researchers, foundation models represent a fundamentally new paradigm in computing. Unlike traditional software that follows explicitly programmed instructions, foundation models learn patterns from vast amounts of data and can be applied to a wide range of tasks.\n\n### Current Limitations and Challenges\n\nDespite their impressive capabilities, today's models face several critical limitations:\n\n- Hallucinations and factual inaccuracies\n- Limited reasoning capabilities\n- Challenges in understanding context and nuance\n- Ethical concerns around bias and representation\n\n### The Path Forward\n\nResearchers highlighted several promising directions for advancing generative AI:\n\n- Improved training methods that require less data\n- Better integration of symbolic reasoning with neural approaches\n- Enhanced techniques for model alignment with human values\n- More sophisticated evaluation frameworks\n\n## Implications for Industry\n\nAs these technologies continue to evolve, we're likely to see profound impacts across virtually every industry - from healthcare and education to creative fields and enterprise software.\n\nFor startups and established companies alike, understanding the trajectory of this research will be essential for making strategic decisions about how to incorporate AI into their products and services.\n\nThe conversation with Stanford researchers underscored that we're still in the early days of generative AI, with tremendous opportunities for innovation ahead.",
    tags: ["Generative AI", "Research", "Stanford", "Foundation Models"],
    readingTime: 6,
  },
  {
    slug: "perspectives-in-generative-ai-with-huggingface",
    title: "Perspectives in Generative AI with Hugging Face's Nazneen Rajani",
    date: "2023-11-15",
    excerpt:
      "A conversation with Hugging Face's lead researcher on evaluation methods, model limitations, and the future of open source in AI.",
    content:
      '# Perspectives in Generative AI with Hugging Face\'s Nazneen Rajani\n\nAs part of our ongoing exploration of the generative AI landscape, I recently had the opportunity to speak with Dr. Nazneen Rajani, Lead Research Scientist at Hugging Face, about the current state and future directions of evaluating and improving large language models.\n\n## Evaluation Challenges\n\nOne of the central themes of our conversation was the difficulty of properly evaluating generative AI systems. As Dr. Rajani pointed out, traditional metrics often fall short when assessing the quality of text generated by today\'s LLMs.\n\n"We need evaluation methods that go beyond simple accuracy metrics and consider factors like factuality, coherence, and alignment with human values," she explained.\n\nHugging Face has been pioneering new approaches to evaluation that provide more nuanced and meaningful assessments of model performance. These include:\n\n- Benchmarks for specific capabilities like reasoning and factual knowledge\n- Human-in-the-loop evaluation frameworks\n- Tools that help identify potential biases and harmful outputs\n\n## The Open Source Advantage\n\nAnother key topic we discussed was the role of open source in advancing AI research and applications.\n\n"Open source creates an environment where models can be tested, critiqued, and improved by a broad community," Dr. Rajani noted. "This leads to more robust, more thoroughly vetted systems."\n\nThe open collaboration model has enabled remarkable progress in areas like:\n\n- Model compression and efficiency\n- Specialized models for specific domains\n- Fine-tuning techniques for improved performance\n- Safety and alignment research\n\n## Looking Ahead\n\nWhen asked about the future of generative AI, Dr. Rajani emphasized the importance of responsible development practices. "As these models become more capable and more widely deployed, we need to ensure they\'re serving human needs in a safe and beneficial way."\n\nShe sees particular promise in approaches that combine the strengths of large foundation models with more specialized components for reasoning, factuality, and domain expertise.\n\nOur conversation highlighted both the tremendous progress the field has made and the substantial challenges that remain as we work to harness the full potential of generative AI.',
    tags: ["Generative AI", "Hugging Face", "Model Evaluation", "Open Source"],
    readingTime: 7,
  },
  {
    slug: "building-enterprise-ai-platforms",
    title: "Building Enterprise AI Platforms: Lessons from the Field",
    date: "2024-03-22",
    excerpt: "Insights on creating robust, scalable AI platforms that balance innovation with enterprise requirements.",
    content:
      '# Building Enterprise AI Platforms: Lessons from the Field\n\nAs organizations across industries embrace AI, many are discovering that success requires more than just implementing individual models or use cases. It demands a thoughtful platform approach that can scale across the enterprise while maintaining security, governance, and performance.\n\nBased on my experience developing AI platforms at both Google and Capital One, I\'d like to share some key lessons for building effective enterprise AI infrastructures.\n\n## Start with Clear Platform Principles\n\nSuccessful AI platforms begin with well-defined principles that guide development decisions:\n\n- **Composability**: Components should work together seamlessly while remaining independently evolvable\n- **Abstraction**: Hide complexity where possible without sacrificing necessary control\n- **Governance**: Build compliance and security into the foundation, not as afterthoughts\n- **Scalability**: Design for enterprise-wide adoption from the beginning\n\n## Balance Standardization and Flexibility\n\nOne of the central tensions in platform development is finding the right balance between standardization and flexibility:\n\n- Too much standardization stifles innovation and creates bottlenecks\n- Too much flexibility leads to fragmentation and duplication of effort\n\nThe best platforms provide strong defaults while allowing justified exceptions. They create clear patterns that make the "right way" the easy way, while still enabling teams to adapt when necessary.\n\n## Focus on the Developer Experience\n\nAI platforms succeed or fail based on adoption, and adoption depends heavily on developer experience:\n\n- Invest in comprehensive documentation and examples\n- Create intuitive APIs and interfaces\n- Build tools that automate common workflows\n- Provide clear paths for getting started and advancing to more complex use cases\n\n## Embrace a Hybrid Approach to Models\n\nEffective AI platforms support multiple approaches to model development and deployment:\n\n- Pre-trained models for common use cases\n- Fine-tuning capabilities for specialized needs\n- Infrastructure for training custom models when necessary\n- Integration with both open and proprietary model providers\n\nThis hybrid approach allows teams to choose the right solution for their specific requirements while maintaining consistency in how models are deployed and monitored.\n\n## Governance as an Enabler, Not a Blocker\n\nIn enterprise environments, governance is non-negotiable. The key is designing governance that enables rather than impedes progress:\n\n- Automate compliance checks and documentation\n- Build monitoring and explainability into core platform services\n- Create clear processes for model validation and deployment\n- Provide tools for data lineage and provenance tracking\n\nWhen done well, governance becomes a competitive advantage rather than a burden, allowing teams to move faster with confidence.\n\n## Conclusion\n\nBuilding enterprise AI platforms is challenging but essential work. By focusing on sound architectural principles, developer experience, and thoughtful governance, organizations can create platforms that enable innovation while meeting enterprise requirements for security, scalability, and compliance.',
    tags: ["Enterprise AI", "Platform Engineering", "ML Ops", "AI Governance"],
    readingTime: 8,
  },
  {
    slug: "kubernetes-to-generative-ai-evolution",
    title: "From Kubernetes to Generative AI: The Evolution of Platform Engineering",
    date: "2024-02-05",
    excerpt: "Reflecting on how lessons from cloud native infrastructure can inform our approach to AI platforms.",
    content:
      "# From Kubernetes to Generative AI: The Evolution of Platform Engineering\n\nThe rapid rise of generative AI has created a new frontier for platform engineering. As someone who worked on Kubernetes and container platforms before moving into AI, I've been struck by both the parallels and the differences between these domains.\n\n## Lessons from Kubernetes That Apply to AI\n\n### 1. Abstraction Levels Matter\n\nKubernetes succeeded because it found the right level of abstraction - high enough to hide infrastructure complexity, but low enough to be flexible. AI platforms face a similar challenge: they need to abstract away model complexity while allowing sufficient customization.\n\nJust as Kubernetes uses concepts like pods and deployments, successful AI platforms need intuitive abstractions that make sense to their users - whether that's prompt templates, model configurations, or evaluation frameworks.\n\n### 2. The Power of Declarative Interfaces\n\nKubernetes' declarative approach - specifying desired state rather than procedures - has proven incredibly powerful. This same pattern works well for AI systems, where specifying what you want (the objective) often works better than detailing how to get there (the implementation).\n\n### 3. The Importance of Community\n\nThe Kubernetes ecosystem thrived because of its vibrant community. Similarly, the most successful AI platforms will be those that foster communities of contributors and users, creating a virtuous cycle of improvement and adoption.\n\n## Where AI Platforms Differ\n\nDespite these parallels, AI platforms present unique challenges:\n\n### 1. Data Centricity\n\nWhile Kubernetes is primarily concerned with compute resources, AI platforms are fundamentally about data - its collection, preparation, and governance. This centrality of data represents a significant shift in platform design priorities.\n\n### 2. Model Opacity\n\nUnlike containers, which execute deterministic code, AI models (especially large neural networks) are inherently opaque. This creates new challenges for debugging, explaining, and ensuring fairness in platform outputs.\n\n### 3. Human-AI Collaboration\n\nAI platforms aren't just about running models; they're about facilitating collaboration between humans and AI systems. This introduces UX considerations that weren't as central to infrastructure platforms.\n\n## Building the Next Generation of AI Platforms\n\nAs we design the next generation of AI platforms, we should carry forward the best lessons from the cloud native era while acknowledging the unique aspects of AI systems:\n\n- **Emphasize observability**: Make the black box of AI as transparent as possible\n- **Design for iteration**: Enable rapid experimentation and learning\n- **Build in guardrails**: Incorporate safety and ethics from the ground up\n- **Focus on productivity**: Measure success by how much the platform amplifies human capabilities\n\nThe transition from infrastructure to AI represents a shift from managing machines to augmenting human intelligence - a profound evolution in how we think about platforms and their purpose.",
    tags: ["Kubernetes", "Platform Engineering", "Generative AI", "Cloud Native"],
    readingTime: 7,
  },
]

export function getAllPosts(): Post[] {
  return posts.sort((a, b) => new Date(b.date).getTime() - new Date(a.date).getTime())
}

export function getPostBySlug(slug: string): Post | undefined {
  return posts.find((post) => post.slug === slug)
}
